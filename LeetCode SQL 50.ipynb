{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "404b20ad-f319-428b-8c03-90b77db993f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+\n",
      "|Emp_Id|Dept|Sal|\n",
      "+------+----+---+\n",
      "|    11|SAMA|100|\n",
      "|    22|  HR|120|\n",
      "|    33|SAMA|110|\n",
      "|    44|SAMA| 90|\n",
      "|    55|  HR| 80|\n",
      "|    66|SAMA|100|\n",
      "+------+----+---+\n",
      "\n",
      "+------+--------+\n",
      "|Emp_Id|Emp_Name|\n",
      "+------+--------+\n",
      "|    11|   Mr. A|\n",
      "|    22|   Mr. B|\n",
      "|    33|   Mr. C|\n",
      "|    44|   Mr. D|\n",
      "|    55|   Mr. E|\n",
      "|    66|   Mr. F|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Question 1\n",
    "\n",
    "Given sample table 1\n",
    "+------+----+---+\n",
    "|Emp_Id|Dept|Sal|\n",
    "+------+----+---+\n",
    "|    11|SAMA|100|\n",
    "|    22|  HR|120|\n",
    "|    33|SAMA|110|\n",
    "|    44|SAMA| 90|\n",
    "|    55|  HR| 80|\n",
    "|    66|SAMA|100|\n",
    "+------+----+---+\n",
    "\n",
    "Table 2\n",
    "+------+--------+\n",
    "|Emp_Id|Emp_Name|\n",
    "+------+--------+\n",
    "|    11|   Mr. A|\n",
    "|    22|   Mr. B|\n",
    "|    33|   Mr. C|\n",
    "|    44|   Mr. D|\n",
    "|    55|   Mr. E|\n",
    "|    66|   Mr. F|\n",
    "+------+--------+\n",
    "\n",
    "Find second highest salry for each department\n",
    "'''\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"CreateDataFrames\") \\\n",
    "    .config(\"spark.pyspark.python\", \"python\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Data for df1\n",
    "data1 = [\n",
    "    (11, \"SAMA\", 100),\n",
    "    (22, \"HR\", 120),\n",
    "    (33, \"SAMA\", 110),\n",
    "    (44, \"SAMA\", 90),\n",
    "    (55, \"HR\", 80),\n",
    "    (66, \"SAMA\", 100)\n",
    "]\n",
    "\n",
    "# Data for df2\n",
    "data2 = [\n",
    "    (11, \"Mr. A\"),\n",
    "    (22, \"Mr. B\"),\n",
    "    (33, \"Mr. C\"),\n",
    "    (44, \"Mr. D\"),\n",
    "    (55, \"Mr. E\"),\n",
    "    (66, \"Mr. F\")\n",
    "]\n",
    "\n",
    "# Column names\n",
    "columns1 = [\"Emp_Id\", \"Dept\", \"Sal\"]\n",
    "columns2 = [\"Emp_Id\", \"Emp_Name\"]\n",
    "\n",
    "# Creating DataFrames\n",
    "df1 = spark.createDataFrame(data1, columns1)\n",
    "df2 = spark.createDataFrame(data2, columns2)\n",
    "\n",
    "# Show the DataFrames\n",
    "df1.show()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7735c48d-bef0-4f64-affe-0490ea7a8e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---+\n",
      "|Dept|Emp_Name|Sal|\n",
      "+----+--------+---+\n",
      "|SAMA|   Mr. A|100|\n",
      "|SAMA|   Mr. F|100|\n",
      "|  HR|   Mr. E| 80|\n",
      "+----+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Solution\n",
    "'''\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as f\n",
    "join_condition = (df1.Emp_Id == df2.Emp_Id)\n",
    "Window_fun=Window.partitionBy(df1.Dept).orderBy(df1.Sal.desc())\n",
    "\n",
    "\n",
    "ordered_df=df1.join(df2,join_condition,\"inner\") \\\n",
    "    .withColumn(\"row_ordering\",f.dense_rank().over(Window_fun)) \\\n",
    "    .drop(df2.Emp_Id)\n",
    "\n",
    "ordered_df \\\n",
    "    .select(\"Dept\",\"Emp_Name\",\"Sal\") \\\n",
    "    .where(ordered_df.row_ordering == 2) \\\n",
    "    .orderBy(ordered_df.Dept.desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64218361-a4fa-4c99-b720-64d6aa422c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+--------+------------+--------------------------+\n",
      "|Emp_Id|Dept|Sal|Emp_Name|row_ordering|mutiplied sal with emp id |\n",
      "+------+----+---+--------+------------+--------------------------+\n",
      "|    22|  HR|120|   Mr. B|           1|                    2640.0|\n",
      "|    55|  HR| 80|   Mr. E|           2|                    4400.0|\n",
      "|    33|SAMA|110|   Mr. C|           1|                    3630.0|\n",
      "|    11|SAMA|100|   Mr. A|           2|                    1100.0|\n",
      "|    66|SAMA|100|   Mr. F|           2|                    6600.0|\n",
      "|    44|SAMA| 90|   Mr. D|           3|                    3960.0|\n",
      "+------+----+---+--------+------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import types as t\n",
    "def sal_emp_multiplier(sal,emp):\n",
    "    return float(sal)*float(emp)\n",
    "\n",
    "mutiplier_udf = f.udf(sal_emp_multiplier, t.FloatType())\n",
    "\n",
    "ordered_df \\\n",
    "    .withColumn(\"mutiplied sal with emp id \",mutiplier_udf(ordered_df.Sal,ordered_df.Emp_Id)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba10f561-d33b-4c8d-b71c-9a58e3f51d56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff7bf23d-1279-4be3-940f-895ede3de85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------------+\n",
      "|tweet_id|content                         |\n",
      "+--------+--------------------------------+\n",
      "|1       |Vote for Biden                  |\n",
      "|2       |Let us make America great again!|\n",
      "+--------+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Write a solution to find the IDs of the invalid tweets. \n",
    "The tweet is invalid if the number of characters used in the content of the tweet is strictly greater than 15.\n",
    "\n",
    "Input: \n",
    "Tweets table:\n",
    "+----------+----------------------------------+\n",
    "| tweet_id | content                          |\n",
    "+----------+----------------------------------+\n",
    "| 1        | Vote for Biden                   |\n",
    "| 2        | Let us make America great again! |\n",
    "+----------+----------------------------------+\n",
    "Output: \n",
    "+----------+\n",
    "| tweet_id |\n",
    "+----------+\n",
    "| 2        |\n",
    "+----------+\n",
    "Explanation: \n",
    "Tweet 1 has length = 14. It is a valid tweet.\n",
    "Tweet 2 has length = 32. It is an invalid tweet.\n",
    "\n",
    "'''\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Tweet Length checker\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = [(1,'Vote for Biden'),(2,'Let us make America great again!')]\n",
    "columns =['tweet_id','content']\n",
    "\n",
    "\n",
    "Tweets_df= spark.createDataFrame(data, columns)\n",
    "\n",
    "Tweets_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d8c7bd9-50d6-415f-ada5-fab2307ceb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|tweet_id|\n",
      "+--------+\n",
      "|       2|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Solution\n",
    "'''\n",
    "\n",
    "Tweets_df \\\n",
    "    .where(f.length(Tweets_df.content)>15) \\\n",
    "    .select(Tweets_df.tweet_id) \\\n",
    "    .distinct() \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a04fbc8-487c-45f0-b360-02d0fa42c9b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "988df315-3c41-4e65-9c40-f7109d1189cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+--------+------+\n",
      "|query_name|          result|position|rating|\n",
      "+----------+----------------+--------+------+\n",
      "|       Dog|Golden Retriever|       1|     5|\n",
      "|       Dog| German Shepherd|       2|     5|\n",
      "|       Dog|            Mule|     200|     1|\n",
      "|       Cat|         Shirazi|       5|     2|\n",
      "|       Cat|         Siamese|       3|     3|\n",
      "|       Cat|          Sphynx|       7|     4|\n",
      "+----------+----------------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Table: Queries\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| query_name  | varchar |\n",
    "| result      | varchar |\n",
    "| position    | int     |\n",
    "| rating      | int     |\n",
    "+-------------+---------+\n",
    "This table may have duplicate rows.\n",
    "This table contains information collected from some queries on a database.\n",
    "The position column has a value from 1 to 500.\n",
    "The rating column has a value from 1 to 5. Query with rating less than 3 is a poor query.\n",
    " \n",
    "\n",
    "We define query quality as:\n",
    "\n",
    "The average of the ratio between query rating and its position.\n",
    "\n",
    "We also define poor query percentage as:\n",
    "\n",
    "The percentage of all queries with rating less than 3.\n",
    "\n",
    "Write a solution to find each query_name, the quality and poor_query_percentage.\n",
    "\n",
    "Both quality and poor_query_percentage should be rounded to 2 decimal places.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Queries table:\n",
    "+------------+-------------------+----------+--------+\n",
    "| query_name | result            | position | rating |\n",
    "+------------+-------------------+----------+--------+\n",
    "| Dog        | Golden Retriever  | 1        | 5      |\n",
    "| Dog        | German Shepherd   | 2        | 5      |\n",
    "| Dog        | Mule              | 200      | 1      |\n",
    "| Cat        | Shirazi           | 5        | 2      |\n",
    "| Cat        | Siamese           | 3        | 3      |\n",
    "| Cat        | Sphynx            | 7        | 4      |\n",
    "+------------+-------------------+----------+--------+\n",
    "Output: \n",
    "+------------+---------+-----------------------+\n",
    "| query_name | quality | poor_query_percentage |\n",
    "+------------+---------+-----------------------+\n",
    "| Dog        | 2.50    | 33.33                 |\n",
    "| Cat        | 0.66    | 33.33                 |\n",
    "+------------+---------+-----------------------+\n",
    "Explanation: \n",
    "Dog queries quality is ((5 / 1) + (5 / 2) + (1 / 200)) / 3 = 2.50\n",
    "Dog queries poor_ query_percentage is (1 / 3) * 100 = 33.33\n",
    "\n",
    "Cat queries quality equals ((2 / 5) + (3 / 3) + (4 / 7)) / 3 = 0.66\n",
    "Cat queries poor_ query_percentage is (1 / 3) * 100 = 33.33\n",
    "\n",
    "'''\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Quality checker\") \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (\"Dog\", \"Golden Retriever\", 1, 5),\n",
    "    (\"Dog\", \"German Shepherd\", 2, 5),\n",
    "    (\"Dog\", \"Mule\", 200, 1),\n",
    "    (\"Cat\", \"Shirazi\", 5, 2),\n",
    "    (\"Cat\", \"Siamese\", 3, 3),\n",
    "    (\"Cat\", \"Sphynx\", 7, 4)\n",
    "]\n",
    "\n",
    "# Define the schema (column names)\n",
    "columns = [\"query_name\", \"result\", \"position\", \"rating\"]\n",
    "\n",
    "Queries_df = spark.createDataFrame(data,columns)\n",
    "\n",
    "Queries_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9f755f3c-9d36-45ed-9454-2dd45b7c4df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------------------+\n",
      "|query_name|quality|poor_query_percentage|\n",
      "+----------+-------+---------------------+\n",
      "|       Dog|    2.5|                33.33|\n",
      "|       Cat|   0.66|                33.33|\n",
      "+----------+-------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Solution\n",
    "'''\n",
    "\n",
    "agg_quality = f.round(f.avg(Queries_df.rating / Queries_df.position ),2).alias('quality')\n",
    "agg_poor_query_percentage = f.round(\n",
    "    (f.sum(f.when(Queries_df.rating < 3, 1).otherwise(0)) / f.count(f.lit(1))) * 100, 2\n",
    ").alias('poor_query_percentage') #use count(\"*\") instead of f.lit(1)\n",
    "\n",
    "'''\n",
    "df_result = Queries_df.groupBy(\"query_name\").agg(\n",
    "    round(avg(col(\"rating\") / col(\"position\")), 2).alias(\"quality\"),\n",
    "    round((sum(when(col(\"rating\") < 3, 1).otherwise(0)) / count(\"*\")) * 100, 2).alias(\"poor_query_percentage\")\n",
    ").show()\n",
    "'''\n",
    "Queries_df \\\n",
    "    .groupBy(Queries_df.query_name) \\\n",
    "    .agg(agg_quality,agg_poor_query_percentage) \\\n",
    "    .where(Queries_df.query_name.isNotNull()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b15d447-e0e8-4f28-9662-574e10375785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ee4bda7-daac-4497-9d1d-a503ccbf72d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+----------+------------+\n",
      "|player_id|device_id|event_date|games_played|\n",
      "+---------+---------+----------+------------+\n",
      "|        1|        2|2016-03-01|           5|\n",
      "|        1|        2|2016-03-02|           6|\n",
      "|        2|        3|2017-06-25|           1|\n",
      "|        3|        1|2016-03-02|           0|\n",
      "|        3|        4|2018-07-03|           5|\n",
      "+---------+---------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Table: Activity\n",
    "\n",
    "+--------------+---------+\n",
    "| Column Name  | Type    |\n",
    "+--------------+---------+\n",
    "| player_id    | int     |\n",
    "| device_id    | int     |\n",
    "| event_date   | date    |\n",
    "| games_played | int     |\n",
    "+--------------+---------+\n",
    "(player_id, event_date) is the primary key (combination of columns with unique values) of this table.\n",
    "This table shows the activity of players of some games.\n",
    "Each row is a record of a player who logged in and played a number of games (possibly 0) before logging out on someday using some device.\n",
    " \n",
    "\n",
    "Write a solution to report the fraction of players that logged in again on the day after the day they first logged in, rounded to 2 decimal places. In other words, you need to count the number of players that logged in for at least two consecutive days starting from their first login date, then divide that number by the total number of players.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Activity table:\n",
    "+-----------+-----------+------------+--------------+\n",
    "| player_id | device_id | event_date | games_played |\n",
    "+-----------+-----------+------------+--------------+\n",
    "| 1         | 2         | 2016-03-01 | 5            |\n",
    "| 1         | 2         | 2016-03-02 | 6            |\n",
    "| 2         | 3         | 2017-06-25 | 1            |\n",
    "| 3         | 1         | 2016-03-02 | 0            |\n",
    "| 3         | 4         | 2018-07-03 | 5            |\n",
    "+-----------+-----------+------------+--------------+\n",
    "Output: \n",
    "+-----------+\n",
    "| fraction  |\n",
    "+-----------+\n",
    "| 0.33      |\n",
    "+-----------+\n",
    "Explanation: \n",
    "Only the player with id 1 logged back in after the first day he had logged in so the answer is 1/3 = 0.33\n",
    "'''\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Second Time Visiter\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"player_id\", IntegerType(), True),\n",
    "    StructField(\"device_id\", IntegerType(), True),\n",
    "    StructField(\"event_date\", StringType(), True),\n",
    "    StructField(\"games_played\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Data\n",
    "data = [\n",
    "    (1, 2, \"2016-03-01\", 5),\n",
    "    (1, 2, \"2016-03-02\", 6),\n",
    "    (2, 3, \"2017-06-25\", 1),\n",
    "    (3, 1, \"2016-03-02\", 0),\n",
    "    (3, 4, \"2018-07-03\", 5)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "Activity_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "Activity_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "622b16ca-2b8d-4497-bbe5-2a5dbc5e14cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Activity_df = Activity_df.withColumn(\"event_date\", f.to_date(Activity_df.event_date, \"yyyy-MM-dd\"))\n",
    "\n",
    "window_lag_fn = Window.partitionBy(Activity_df.player_id).orderBy(Activity_df.event_date.asc())\n",
    "\n",
    "Activity_ordered_df=Activity_df \\\n",
    "    .withColumn(\"lead_date\",f.lead(Activity_df.event_date).over(window_lag_fn)) \\\n",
    "    .withColumn(\"event_number\",f.row_number().over(window_lag_fn)) \\\n",
    "    .select(\"player_id\",\"event_date\",\"lead_date\",\"event_number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7844c815-a735-4bc0-a8a0-855743718f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+--------+\n",
      "|distinct_players|distinct_2nd_login|fraction|\n",
      "+----------------+------------------+--------+\n",
      "|               3|                 1|    0.33|\n",
      "+----------------+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_distinct_players = f.countDistinct(Activity_ordered_df.player_id).alias(\"distinct_players\")\n",
    "count_distinct_2nd_login_players = f.countDistinct(f.when( (Activity_ordered_df.event_date == Activity_ordered_df.lead_date-1),\n",
    "                                                  Activity_ordered_df.player_id)).alias(\"distinct_2nd_login\")\n",
    "\n",
    "agg_data_df =Activity_ordered_df \\\n",
    "    .where(Activity_ordered_df.event_number == 1) \\\n",
    "    .agg(count_distinct_players,count_distinct_2nd_login_players)\n",
    "\n",
    "agg_data_df \\\n",
    "    .withColumn(\"fraction\",f.round(agg_data_df.distinct_2nd_login/agg_data_df.distinct_players,2)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91018efb-7bc6-446d-997b-a35f21927b87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01ca4e5d-2ded-4866-acd1-651958a57e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----+--------+-----+\n",
      "|sale_id|product_id|year|quantity|price|\n",
      "+-------+----------+----+--------+-----+\n",
      "|      1|       100|2008|      10| 5000|\n",
      "|      2|       100|2009|      12| 5000|\n",
      "|      7|       200|2011|      15| 9000|\n",
      "+-------+----------+----+--------+-----+\n",
      "\n",
      "+----------+------------+\n",
      "|product_id|product_name|\n",
      "+----------+------------+\n",
      "|       100|       Nokia|\n",
      "|       200|       Apple|\n",
      "|       300|     Samsung|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Table: Sales\n",
    "\n",
    "+-------------+-------+\n",
    "| Column Name | Type  |\n",
    "+-------------+-------+\n",
    "| sale_id     | int   |\n",
    "| product_id  | int   |\n",
    "| year        | int   |\n",
    "| quantity    | int   |\n",
    "| price       | int   |\n",
    "+-------------+-------+\n",
    "(sale_id, year) is the primary key (combination of columns with unique values) of this table.\n",
    "product_id is a foreign key (reference column) to Product table.\n",
    "Each row of this table shows a sale on the product product_id in a certain year.\n",
    "Note that the price is per unit.\n",
    " \n",
    "\n",
    "Table: Product\n",
    "\n",
    "+--------------+---------+\n",
    "| Column Name  | Type    |\n",
    "+--------------+---------+\n",
    "| product_id   | int     |\n",
    "| product_name | varchar |\n",
    "+--------------+---------+\n",
    "product_id is the primary key (column with unique values) of this table.\n",
    "Each row of this table indicates the product name of each product.\n",
    "\n",
    "Write a solution to select the product id, year, quantity, and price for the first year of every product sold.\n",
    "Return the resulting table in any order.\n",
    "The result format is in the following example.\n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Sales table:\n",
    "+---------+------------+------+----------+-------+\n",
    "| sale_id | product_id | year | quantity | price |\n",
    "+---------+------------+------+----------+-------+ \n",
    "| 1       | 100        | 2008 | 10       | 5000  |\n",
    "| 2       | 100        | 2009 | 12       | 5000  |\n",
    "| 7       | 200        | 2011 | 15       | 9000  |\n",
    "+---------+------------+------+----------+-------+\n",
    "Product table:\n",
    "+------------+--------------+\n",
    "| product_id | product_name |\n",
    "+------------+--------------+\n",
    "| 100        | Nokia        |\n",
    "| 200        | Apple        |\n",
    "| 300        | Samsung      |\n",
    "+------------+--------------+\n",
    "Output: \n",
    "+------------+------------+----------+-------+\n",
    "| product_id | first_year | quantity | price |\n",
    "+------------+------------+----------+-------+ \n",
    "| 100        | 2008       | 10       | 5000  |\n",
    "| 200        | 2011       | 15       | 9000  |\n",
    "+------------+------------+----------+-------+\n",
    "'''\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"First year data selector\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sales_data = [\n",
    "    (1, 100, 2008, 10, 5000),\n",
    "    (2, 100, 2009, 12, 5000),\n",
    "    (7, 200, 2011, 15, 9000)\n",
    "]\n",
    "sales_columns = [\"sale_id\", \"product_id\", \"year\", \"quantity\", \"price\"]\n",
    "sales_df = spark.createDataFrame(sales_data, sales_columns)\n",
    "# Show Sales DataFrame\n",
    "sales_df.show()\n",
    "\n",
    "\n",
    "product_data = [\n",
    "    (100, \"Nokia\"),\n",
    "    (200, \"Apple\"),\n",
    "    (300, \"Samsung\")\n",
    "]\n",
    "product_columns = [\"product_id\", \"product_name\"]\n",
    "product_df = spark.createDataFrame(product_data, product_columns)\n",
    "# Show Product DataFrame\n",
    "product_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6629129-56d5-4cfc-9335-aa668231c404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+-----+\n",
      "|product_id|first_year|quantity|price|\n",
      "+----------+----------+--------+-----+\n",
      "|       100|      2008|      10| 5000|\n",
      "|       200|      2011|      15| 9000|\n",
      "+----------+----------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Solution\n",
    "'''\n",
    "window_expr = Window.partitionBy(sales_df.product_id).orderBy(sales_df.year.asc())\n",
    "order_sales_df = sales_df \\\n",
    "    .withColumn(\"ordering\",f.dense_rank().over(window_expr)) \\\n",
    "    .withColumnRenamed(\"year\",\"first_year\")\n",
    "\n",
    "order_sales_df \\\n",
    "    .where(order_sales_df.ordering == 1) \\\n",
    "    .select(order_sales_df.product_id,order_sales_df.first_year,order_sales_df.quantity,order_sales_df.price) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c2b71d-ac54-44d4-b845-8714b99d3dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b842a5e-b187-400b-af25-d2fa35b4d882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|    name|\n",
      "+---+--------+\n",
      "|  1|   Alice|\n",
      "|  7|     Bob|\n",
      "| 11|    Meir|\n",
      "| 90| Winston|\n",
      "|  3|Jonathan|\n",
      "+---+--------+\n",
      "\n",
      "+---+---------+\n",
      "| id|unique_id|\n",
      "+---+---------+\n",
      "|  3|        1|\n",
      "| 11|        2|\n",
      "| 90|        3|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Table: Employees\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| id            | int     |\n",
    "| name          | varchar |\n",
    "+---------------+---------+\n",
    "id is the primary key (column with unique values) for this table.\n",
    "Each row of this table contains the id and the name of an employee in a company.\n",
    " \n",
    "\n",
    "Table: EmployeeUNI\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| id            | int     |\n",
    "| unique_id     | int     |\n",
    "+---------------+---------+\n",
    "(id, unique_id) is the primary key (combination of columns with unique values) for this table.\n",
    "Each row of this table contains the id and the corresponding unique id of an employee in the company.\n",
    " \n",
    "\n",
    "Write a solution to show the unique ID of each user, If a user does not have a unique ID replace just show null.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Employees table:\n",
    "+----+----------+\n",
    "| id | name     |\n",
    "+----+----------+\n",
    "| 1  | Alice    |\n",
    "| 7  | Bob      |\n",
    "| 11 | Meir     |\n",
    "| 90 | Winston  |\n",
    "| 3  | Jonathan |\n",
    "+----+----------+\n",
    "EmployeeUNI table:\n",
    "+----+-----------+\n",
    "| id | unique_id |\n",
    "+----+-----------+\n",
    "| 3  | 1         |\n",
    "| 11 | 2         |\n",
    "| 90 | 3         |\n",
    "+----+-----------+\n",
    "Output: \n",
    "+-----------+----------+\n",
    "| unique_id | name     |\n",
    "+-----------+----------+\n",
    "| null      | Alice    |\n",
    "| null      | Bob      |\n",
    "| 2         | Meir     |\n",
    "| 3         | Winston  |\n",
    "| 1         | Jonathan |\n",
    "+-----------+----------+\n",
    "Explanation: \n",
    "Alice and Bob do not have a unique ID, We will show null instead.\n",
    "The unique ID of Meir is 2.\n",
    "The unique ID of Winston is 3.\n",
    "The unique ID of Jonathan is 1.\n",
    "\n",
    "'''\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Unique ID finder\") \\\n",
    "        .master(\"local[2]\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "data_1 =[(1,\"Alice\"),(7,\"Bob\"),(11,\"Meir\"),(90,\"Winston\"),(3,\"Jonathan\")]\n",
    "columns_1 =[\"id\",\"name\"]\n",
    "\n",
    "Employees_df = spark.createDataFrame(data_1,columns_1)\n",
    "Employees_df.show()\n",
    "\n",
    "\n",
    "data_2 =[(3,1),(11,2),(90,3)]\n",
    "columns_2 =[\"id\",\"unique_id\"]\n",
    "\n",
    "EmployeeUNI_df = spark.createDataFrame(data_2,columns_2)\n",
    "EmployeeUNI_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f430c8a5-d97e-482f-a985-84c16363277c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|unique_id|    name|\n",
      "+---------+--------+\n",
      "|     NULL|     Bob|\n",
      "|     NULL|   Alice|\n",
      "|        1|Jonathan|\n",
      "|        2|    Meir|\n",
      "|        3| Winston|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Employees_df \\\n",
    "    .join(EmployeeUNI_df,Employees_df.id == EmployeeUNI_df.id,\"left\") \\\n",
    "    .select(EmployeeUNI_df.unique_id,Employees_df.name) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726ccfd9-086d-423a-8680-62d910670571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfb451e1-a762-49f6-8960-f9723abfb8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----+--------+-----+\n",
      "|sale_id|product_id|year|quantity|price|\n",
      "+-------+----------+----+--------+-----+\n",
      "|      1|       100|2008|      10| 5000|\n",
      "|      2|       100|2009|      12| 5000|\n",
      "|      7|       200|2011|      15| 9000|\n",
      "+-------+----------+----+--------+-----+\n",
      "\n",
      "+----------+------------+\n",
      "|product_id|product_name|\n",
      "+----------+------------+\n",
      "|       100|       Nokia|\n",
      "|       200|       Apple|\n",
      "|       300|     Samsung|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Table: Sales\n",
    "\n",
    "+-------------+-------+\n",
    "| Column Name | Type  |\n",
    "+-------------+-------+\n",
    "| sale_id     | int   |\n",
    "| product_id  | int   |\n",
    "| year        | int   |\n",
    "| quantity    | int   |\n",
    "| price       | int   |\n",
    "+-------------+-------+\n",
    "(sale_id, year) is the primary key (combination of columns with unique values) of this table.\n",
    "product_id is a foreign key (reference column) to Product table.\n",
    "Each row of this table shows a sale on the product product_id in a certain year.\n",
    "Note that the price is per unit.\n",
    " \n",
    "\n",
    "Table: Product\n",
    "\n",
    "+--------------+---------+\n",
    "| Column Name  | Type    |\n",
    "+--------------+---------+\n",
    "| product_id   | int     |\n",
    "| product_name | varchar |\n",
    "+--------------+---------+\n",
    "product_id is the primary key (column with unique values) of this table.\n",
    "Each row of this table indicates the product name of each product.\n",
    " \n",
    "\n",
    "Write a solution to report the product_name, year, and price for each sale_id in the Sales table.\n",
    "\n",
    "Return the resulting table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Sales table:\n",
    "+---------+------------+------+----------+-------+\n",
    "| sale_id | product_id | year | quantity | price |\n",
    "+---------+------------+------+----------+-------+ \n",
    "| 1       | 100        | 2008 | 10       | 5000  |\n",
    "| 2       | 100        | 2009 | 12       | 5000  |\n",
    "| 7       | 200        | 2011 | 15       | 9000  |\n",
    "+---------+------------+------+----------+-------+\n",
    "Product table:\n",
    "+------------+--------------+\n",
    "| product_id | product_name |\n",
    "+------------+--------------+\n",
    "| 100        | Nokia        |\n",
    "| 200        | Apple        |\n",
    "| 300        | Samsung      |\n",
    "+------------+--------------+\n",
    "Output: \n",
    "+--------------+-------+-------+\n",
    "| product_name | year  | price |\n",
    "+--------------+-------+-------+\n",
    "| Nokia        | 2008  | 5000  |\n",
    "| Nokia        | 2009  | 5000  |\n",
    "| Apple        | 2011  | 9000  |\n",
    "+--------------+-------+-------+\n",
    "Explanation: \n",
    "From sale_id = 1, we can conclude that Nokia was sold for 5000 in the year 2008.\n",
    "From sale_id = 2, we can conclude that Nokia was sold for 5000 in the year 2009.\n",
    "From sale_id = 7, we can conclude that Apple was sold for 9000 in the year 2011.\n",
    "'''\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"SalesAndProduct\").getOrCreate()\n",
    "\n",
    "# Create the Sales DataFrame\n",
    "sales_data = [\n",
    "    Row(sale_id=1, product_id=100, year=2008, quantity=10, price=5000),\n",
    "    Row(sale_id=2, product_id=100, year=2009, quantity=12, price=5000),\n",
    "    Row(sale_id=7, product_id=200, year=2011, quantity=15, price=9000)\n",
    "]\n",
    "\n",
    "sales_df = spark.createDataFrame(sales_data)\n",
    "\n",
    "# Create the Product DataFrame\n",
    "product_data = [\n",
    "    Row(product_id=100, product_name='Nokia'),\n",
    "    Row(product_id=200, product_name='Apple'),\n",
    "    Row(product_id=300, product_name='Samsung')\n",
    "]\n",
    "\n",
    "product_df = spark.createDataFrame(product_data)\n",
    "\n",
    "# Show the data in both DataFrames\n",
    "sales_df.show()\n",
    "product_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b1d5a3a-e650-429f-b72e-5e12dae80980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+-----+\n",
      "|product_name|year|price|\n",
      "+------------+----+-----+\n",
      "|       Nokia|2008| 5000|\n",
      "|       Nokia|2009| 5000|\n",
      "|       Apple|2011| 9000|\n",
      "+------------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "+--------------+-------+-------+\n",
    "| product_name | year  | price |\n",
    "+--------------+-------+-------+\n",
    "| Nokia        | 2008  | 5000  |\n",
    "| Nokia        | 2009  | 5000  |\n",
    "| Apple        | 2011  | 9000  |\n",
    "+--------------+-------+-------+\n",
    "\"\"\"\n",
    "\n",
    "sales_df \\\n",
    "    .join(product_df,sales_df.product_id == product_df.product_id,\"left\") \\\n",
    "    .select(product_df.product_name,sales_df.year,sales_df.price) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0cd07b-072e-4a31-8e40-867263013a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b213585f-e176-4c47-90d0-b49a47bcef5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|num|\n",
      "+---+---+\n",
      "|  1|  1|\n",
      "|  2|  1|\n",
      "|  3|  1|\n",
      "|  4|  2|\n",
      "|  5|  1|\n",
      "|  6|  2|\n",
      "|  7|  2|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Table: Logs\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| id          | int     |\n",
    "| num         | varchar |\n",
    "+-------------+---------+\n",
    "In SQL, id is the primary key for this table.\n",
    "id is an autoincrement column starting from 1.\n",
    " \n",
    "\n",
    "Find all numbers that appear at least three times consecutively.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Logs table:\n",
    "+----+-----+\n",
    "| id | num |\n",
    "+----+-----+\n",
    "| 1  | 1   |\n",
    "| 2  | 1   |\n",
    "| 3  | 1   |\n",
    "| 4  | 2   |\n",
    "| 5  | 1   |\n",
    "| 6  | 2   |\n",
    "| 7  | 2   |\n",
    "+----+-----+\n",
    "Output: \n",
    "+-----------------+\n",
    "| ConsecutiveNums |\n",
    "+-----------------+\n",
    "| 1               |\n",
    "+-----------------+\n",
    "Explanation: 1 is the only number that appears consecutively for at least three times.\n",
    "'''\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"SimpleTable\").getOrCreate()\n",
    "\n",
    "# Create the DataFrame for the given table without using Row class\n",
    "data = [\n",
    "    {'id': 1, 'num': 1},\n",
    "    {'id': 2, 'num': 1},\n",
    "    {'id': 3, 'num': 1},\n",
    "    {'id': 4, 'num': 2},\n",
    "    {'id': 5, 'num': 1},\n",
    "    {'id': 6, 'num': 2},\n",
    "    {'id': 7, 'num': 2}\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe82a0c8-46d8-41bc-bed6-80063ef52432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|ConsecutiveNums|\n",
      "+---------------+\n",
      "|              1|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.orderBy(df.id.asc())\n",
    "ordered_df = df \\\n",
    "    .withColumn(\"lagged_one\",f.lag(df.num,1).over(window_spec)) \\\n",
    "    .withColumn(\"lagged_two\",f.lag(df.num,2).over(window_spec))\n",
    "\n",
    "ordered_df \\\n",
    "    .where((ordered_df.lagged_one == ordered_df.lagged_two) & (ordered_df.lagged_one == ordered_df.num)) \\\n",
    "    .withColumnRenamed(\"num\",\"ConsecutiveNums\") \\\n",
    "    .select(\"ConsecutiveNums\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552d9e91-8772-46fe-8d0f-a708acaf613c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f543b7e-cc67-4924-9e8c-b877e974235c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+---------+\n",
      "| id| name|department|managerId|\n",
      "+---+-----+----------+---------+\n",
      "|101| John|         A|     NULL|\n",
      "|102|  Dan|         A|      101|\n",
      "|103|James|         A|      101|\n",
      "|104|  Amy|         A|      101|\n",
      "|105| Anne|         A|      101|\n",
      "|106|  Ron|         B|      101|\n",
      "+---+-----+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Table: Employee\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| id          | int     |\n",
    "| name        | varchar |\n",
    "| department  | varchar |\n",
    "| managerId   | int     |\n",
    "+-------------+---------+\n",
    "id is the primary key (column with unique values) for this table.\n",
    "Each row of this table indicates the name of an employee, their department, and the id of their manager.\n",
    "If managerId is null, then the employee does not have a manager.\n",
    "No employee will be the manager of themself.\n",
    " \n",
    "\n",
    "Write a solution to find managers with at least five direct reports.\n",
    "Return the result table in any order.\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Employee table:\n",
    "+-----+-------+------------+-----------+\n",
    "| id  | name  | department | managerId |\n",
    "+-----+-------+------------+-----------+\n",
    "| 101 | John  | A          | null      |\n",
    "| 102 | Dan   | A          | 101       |\n",
    "| 103 | James | A          | 101       |\n",
    "| 104 | Amy   | A          | 101       |\n",
    "| 105 | Anne  | A          | 101       |\n",
    "| 106 | Ron   | B          | 101       |\n",
    "+-----+-------+------------+-----------+\n",
    "Output: \n",
    "+------+\n",
    "| name |\n",
    "+------+\n",
    "| John |\n",
    "+------+\n",
    "'''\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"DataFrameExample\").getOrCreate()\n",
    "\n",
    "# Define schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"managerId\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Define the data\n",
    "data = [\n",
    "    (101, 'John', 'A', None),\n",
    "    (102, 'Dan', 'A', 101),\n",
    "    (103, 'James', 'A', 101),\n",
    "    (104, 'Amy', 'A', 101),\n",
    "    (105, 'Anne', 'A', 101),\n",
    "    (106, 'Ron', 'B', 101)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "Employee_df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "Employee_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cac55c14-bea4-4f2d-9e01-199901263b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "| mId|tcount|\n",
      "+----+------+\n",
      "|NULL|     1|\n",
      "| 101|     5|\n",
      "+----+------+\n",
      "\n",
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|John|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "agg_Employee_df = Employee_df \\\n",
    "                    .groupBy(Employee_df.managerId) \\\n",
    "                    .agg(f.count(\"*\").alias(\"tcount\")) \\\n",
    "                    .withColumnRenamed(\"managerId\",\"mId\")\n",
    "\n",
    "agg_Employee_df.show()\n",
    "\n",
    "result_df = agg_Employee_df.join(Employee_df, Employee_df.id == agg_Employee_df.mId, \"inner\") \\\n",
    "            .drop(Employee_df.managerId) \\\n",
    "            .where (agg_Employee_df.tcount>= 5)\n",
    "\n",
    "result_df \\\n",
    "    .select(result_df.name) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd65d40a-b482-44e6-84f1-50058dee1294",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
